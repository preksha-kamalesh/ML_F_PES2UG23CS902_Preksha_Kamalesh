{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":309628,"datasetId":107582,"databundleVersionId":322619,"isSourceIdPinned":false}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Week 14: CNN Lab - Rock, Paper, Scissors\n\n**Objective:** Build, train, and test a Convolutional Neural Network (CNN) to classify images of hands playing Rock, Paper, or Scissors.","metadata":{"id":"intro_cell"}},{"cell_type":"markdown","source":"### Step 1: Setup and Data Download\n\nThis first cell downloads the dataset from Kaggle.","metadata":{"id":"step_1_md"}},{"cell_type":"code","source":"import kagglehub\n\npath = kagglehub.dataset_download(\"drgfreeman/rockpaperscissors\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"id":"DHpeGiatx8yO","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:14:31.310595Z","iopub.execute_input":"2025-11-19T14:14:31.310878Z","iopub.status.idle":"2025-11-19T14:14:37.029017Z","shell.execute_reply.started":"2025-11-19T14:14:31.310850Z","shell.execute_reply":"2025-11-19T14:14:37.028048Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/rockpaperscissors\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\nimport os\n\nsrc_root = \"/kaggle/input/rockpaperscissors\"\ndst_root = \"/content/dataset\"\n\nos.makedirs(dst_root, exist_ok=True)\n\nfolders_to_copy = [\"rock\", \"paper\", \"scissors\"]\n\nfor folder in folders_to_copy:\n    src_path = os.path.join(src_root, folder)\n    dst_path = os.path.join(dst_root, folder)\n\n    if os.path.exists(src_path):\n        shutil.copytree(src_path, dst_path, dirs_exist_ok=True)\n        print(\"Copied:\", folder)\n    else:\n        print(\"Folder not found:\", folder)\n\n","metadata":{"id":"3SVJhchl2XCb","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:14:41.100947Z","iopub.execute_input":"2025-11-19T14:14:41.101287Z","iopub.status.idle":"2025-11-19T14:15:03.246295Z","shell.execute_reply.started":"2025-11-19T14:14:41.101263Z","shell.execute_reply":"2025-11-19T14:15:03.245504Z"}},"outputs":[{"name":"stdout","text":"Copied: rock\nCopied: paper\nCopied: scissors\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Step 2: Imports and Device Setup\n\nImport the necessary libraries and check if a GPU is available.","metadata":{"id":"step_2_md"}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, random_split\nfrom PIL import Image\nimport numpy as np\n\n# TODO: Set the 'device' variable\n# Check if CUDA (GPU) is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Using device:\", device)","metadata":{"id":"1n2gYN8TyydM","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:15:06.274891Z","iopub.execute_input":"2025-11-19T14:15:06.275309Z","iopub.status.idle":"2025-11-19T14:15:14.228891Z","shell.execute_reply.started":"2025-11-19T14:15:06.275278Z","shell.execute_reply":"2025-11-19T14:15:14.228067Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"### Step 3: Data Loading and Preprocessing\n\nHere we will define our image transformations, load the dataset, split it, and create DataLoaders.","metadata":{"id":"step_3_md"}},{"cell_type":"code","source":"DATA_DIR = \"/content/dataset\"\n\n\n# TODO: Define the image transforms\n# We need to:\n# 1. Resize all images to 128x128\n# 2. Convert them to Tensors\n# 3. Normalize them (mean=0.5, std=0.5)\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n\n# Load dataset using ImageFolder\nfull_dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n\nclass_names = full_dataset.classes\nprint(\"Classes:\", class_names)\n\n# TODO: Split the dataset\n# We want 80% for training and 20% for testing\ntrain_size = int(0.8 * len(full_dataset))  # <-- Calculate 80% of len(full_dataset)\ntest_size = len(full_dataset) - train_size  # <-- Remainder\n\n# TODO: Use random_split to create train_dataset and test_dataset\ntrain_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n\n\n# TODO: Create the DataLoaders\n# Use a batch_size of 32\n# Shuffle the training loader, but not the test loader\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nprint(f\"Total images: {len(full_dataset)}\")\nprint(f\"Training images: {len(train_dataset)}\")\nprint(f\"Test images: {len(test_dataset)}\")","metadata":{"id":"SkJ5XlSDy0HF","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:15:20.560998Z","iopub.execute_input":"2025-11-19T14:15:20.561487Z","iopub.status.idle":"2025-11-19T14:15:20.581991Z","shell.execute_reply.started":"2025-11-19T14:15:20.561461Z","shell.execute_reply":"2025-11-19T14:15:20.580366Z"}},"outputs":[{"name":"stdout","text":"Classes: ['paper', 'rock', 'scissors']\nTotal images: 2188\nTraining images: 1750\nTest images: 438\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Step 4: Define the CNN Model\n\nFill in the `conv_block` and `fc_block` with the correct layers.","metadata":{"id":"step_4_md"}},{"cell_type":"code","source":"class RPS_CNN(nn.Module):\n    def __init__(self):\n        super(RPS_CNN, self).__init__()\n\n        # TODO: Define the convolutional block\n        # We want 3 blocks:\n        # 1. Conv2d(3 -> 16 channels, kernel=3, padding=1), ReLU, MaxPool2d(2)\n        # 2. Conv2d(16 -> 32 channels, kernel=3, padding=1), ReLU, MaxPool2d(2)\n        # 3. Conv2d(32 -> 64 channels, kernel=3, padding=1), ReLU, MaxPool2d(2)\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n\n        # After 3 MaxPool(2) layers, our 128x128 image becomes:\n        # 128 -> 64 -> 32 -> 16\n        # So the flattened size is 64 * 16 * 16\n\n        # TODO: Define the fully-connected (classifier) block\n        # We want:\n        # 1. Flatten the input\n        # 2. Linear layer (64 * 16 * 16 -> 256)\n        # 3. ReLU\n        # 4. Dropout (p=0.3)\n        # 5. Linear layer (256 -> 3) (3 classes: rock, paper, scissors)\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(64 * 16 * 16, 256),\n            nn.ReLU(),\n            nn.Dropout(p=0.3),\n            nn.Linear(256, 3)\n        )\n\n    def forward(self, x):\n        x = self.conv_block(x)\n        x = self.fc(x)\n        return x\n\n# TODO: Initialize the model, criterion, and optimizer\n# 1. Create an instance of RPS_CNN and move it to the 'device'\nmodel = RPS_CNN().to(device)  # <-- Replace this\n\n# 2. Define the loss function (Criterion). Use CrossEntropyLoss for classification.\ncriterion = nn.CrossEntropyLoss()  # <-- Replace this\n\n# 3. Define the optimizer. Use Adam with a learning rate of 0.001\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # <-- Replace this\n\nprint(model)","metadata":{"id":"wexPK8V3y3Fx","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:15:28.421439Z","iopub.execute_input":"2025-11-19T14:15:28.422192Z","iopub.status.idle":"2025-11-19T14:15:28.482380Z","shell.execute_reply.started":"2025-11-19T14:15:28.422151Z","shell.execute_reply":"2025-11-19T14:15:28.481507Z"}},"outputs":[{"name":"stdout","text":"RPS_CNN(\n  (conv_block): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU()\n    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=16384, out_features=256, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.3, inplace=False)\n    (4): Linear(in_features=256, out_features=3, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Step 5: Train the Model\n\nFill in the core training steps inside the loop.","metadata":{"id":"step_5_md"}},{"cell_type":"code","source":"EPOCHS = 10\n\nfor epoch in range(EPOCHS):\n    model.train() # Set the model to training mode\n    total_loss = 0\n\n    for images, labels in train_loader:\n        # Move data to the correct device\n        images, labels = images.to(device), labels.to(device)\n\n        # TODO: Implement the training steps\n        # 1. Clear the gradients (optimizer.zero_grad())\n        optimizer.zero_grad()\n\n        # 2. Perform a forward pass (get model outputs)\n        outputs = model(images)\n\n        # 3. Calculate the loss (using criterion)\n        loss = criterion(outputs, labels)\n\n        # 4. Perform a backward pass (loss.backward())\n        loss.backward()\n\n        # 5. Update the weights (optimizer.step())\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss = {total_loss/len(train_loader):.4f}\")\n\nprint(\"Training complete!\")","metadata":{"id":"L_GqO57IzQLs","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:15:31.944487Z","iopub.execute_input":"2025-11-19T14:15:31.944770Z","iopub.status.idle":"2025-11-19T14:18:52.135169Z","shell.execute_reply.started":"2025-11-19T14:15:31.944750Z","shell.execute_reply":"2025-11-19T14:18:52.134198Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10, Loss = 0.6433\nEpoch 2/10, Loss = 0.1575\nEpoch 3/10, Loss = 0.0826\nEpoch 4/10, Loss = 0.0629\nEpoch 5/10, Loss = 0.0414\nEpoch 6/10, Loss = 0.0122\nEpoch 7/10, Loss = 0.0059\nEpoch 8/10, Loss = 0.0094\nEpoch 9/10, Loss = 0.0077\nEpoch 10/10, Loss = 0.0172\nTraining complete!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Step 6: Evaluate the Model\n\nTest the model's accuracy on the unseen test set.","metadata":{"id":"step_6_md"}},{"cell_type":"code","source":"model.eval() # Set the model to evaluation mode\ncorrect = 0\ntotal = 0\n\n# TODO: Use torch.no_grad()\n# We don't need to calculate gradients during evaluation\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        # TODO: Get model predictions\n        # 1. Get the raw model outputs (logits)\n        outputs = model(images)  # <-- Replace this\n\n        # 2. Get the predicted class (the one with the highest score)\n        #    Hint: use torch.max(outputs, 1)\n        _, predicted = torch.max(outputs, 1)  # <-- Replace this\n\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n","metadata":{"id":"iYCNxBrjzU1t","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:20:46.664966Z","iopub.execute_input":"2025-11-19T14:20:46.665341Z","iopub.status.idle":"2025-11-19T14:20:49.780817Z","shell.execute_reply.started":"2025-11-19T14:20:46.665315Z","shell.execute_reply":"2025-11-19T14:20:49.779982Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 97.72%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Step 7: Test on a Single Image\n\nLet's see how the model performs on one image.","metadata":{"id":"step_7_md"}},{"cell_type":"code","source":"def predict_image(model, img_path):\n    model.eval()\n\n    img = Image.open(img_path).convert(\"RGB\")\n    # Apply the same transforms as training, and add a batch dimension (unsqueeze)\n    img = transform(img).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        # TODO: Get the model prediction\n        # 1. Get the raw model outputs (logits)\n        output = model(img)  # <-- Replace this\n\n        # 2. Get the predicted class index\n        _, pred = torch.max(output, 1)  # <-- Replace this\n\n    return class_names[pred.item()]\n\n# Test the function (this path should exist)\ntest_img_path = \"/kaggle/input/rockpaperscissors/paper/0Uomd0HvOB33m47I.png\"\nprediction = predict_image(model, test_img_path)\nprint(f\"Model prediction for {test_img_path}: {prediction}\")","metadata":{"id":"RN00Dkw9zceh","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:22:04.559993Z","iopub.execute_input":"2025-11-19T14:22:04.560729Z","iopub.status.idle":"2025-11-19T14:22:04.599719Z","shell.execute_reply.started":"2025-11-19T14:22:04.560702Z","shell.execute_reply":"2025-11-19T14:22:04.598713Z"}},"outputs":[{"name":"stdout","text":"Model prediction for /kaggle/input/rockpaperscissors/paper/0Uomd0HvOB33m47I.png: paper\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"### Step 8: Play the Game!\n\nThis code is complete. If your model is trained, you can run this cell to have the model play against itself.","metadata":{"id":"step_8_md"}},{"cell_type":"code","source":"import random\nimport os\n\ndef pick_random_image(class_name):\n    folder = f\"/content/dataset/{class_name}\"\n    files = os.listdir(folder)\n    img = random.choice(files)\n    return os.path.join(folder, img)\n\ndef rps_winner(move1, move2):\n    if move1 == move2:\n        return \"Draw\"\n\n    rules = {\n        \"rock\": \"scissors\",\n        \"paper\": \"rock\",\n        \"scissors\": \"paper\"\n    }\n\n    if rules[move1] == move2:\n        return f\"Player 1 wins! {move1} beats {move2}\"\n    else:\n        return f\"Player 2 wins! {move2} beats {move1}\"\n\n\n# -----------------------------------------------------------\n# 1. Choose any two random classes\n# -----------------------------------------------------------\n\nchoices = [\"rock\", \"paper\", \"scissors\"]\nc1 = random.choice(choices)\nc2 = random.choice(choices)\n\nimg1_path = pick_random_image(c1)\nimg2_path = pick_random_image(c2)\n\nprint(\"Randomly selected images:\")\nprint(\"Image 1:\", img1_path)\nprint(\"Image 2:\", img2_path)\n\n\n# -----------------------------------------------------------\n# 2. Predict their labels using the model\n# -----------------------------------------------------------\n\np1 = predict_image(model, img1_path)\np2 = predict_image(model, img2_path)\n\nprint(\"\\nPlayer 1 shows:\", p1)\nprint(\"Player 2 shows:\", p2)\n\n# -----------------------------------------------------------\n# 3. Decide the winner\n# -----------------------------------------------------------\n\nprint(\"\\nRESULT:\", rps_winner(p1, p2))","metadata":{"id":"13-RXEbuzuxu","trusted":true,"execution":{"iopub.status.busy":"2025-11-19T14:22:23.232337Z","iopub.execute_input":"2025-11-19T14:22:23.232961Z","iopub.status.idle":"2025-11-19T14:22:23.264686Z","shell.execute_reply.started":"2025-11-19T14:22:23.232932Z","shell.execute_reply":"2025-11-19T14:22:23.263643Z"}},"outputs":[{"name":"stdout","text":"Randomly selected images:\nImage 1: /content/dataset/rock/AydtaubqlFzR1W5x.png\nImage 2: /content/dataset/scissors/jbj5K46TOmR5hkWu.png\n\nPlayer 1 shows: rock\nPlayer 2 shows: scissors\n\nRESULT: Player 1 wins! rock beats scissors\n","output_type":"stream"}],"execution_count":10}]}